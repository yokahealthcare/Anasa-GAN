{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/yokahealthcare/Anasa-GAN/blob/master/%5BMain%20v2%5D%20%5BAE%5D%20%5BANN%5D%20Breathing_Wave.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mglGdAa9EXAw"
   },
   "source": [
    "# AE - AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bkqs7_K0UFuK"
   },
   "source": [
    "## Project Strucuture\n",
    "\n",
    "### PART 1 : Data Preprocessing\n",
    "\n",
    "\n",
    "1.   Filter the zeros values (except if in the first column)\n",
    "2.   Separate the data(q) according to labels\n",
    "3.   Seperate q into data(X) and label(Y)\n",
    "4.   Normalize the data\n",
    "> X normalized using MinMaxScaler between 0 and 1\n",
    ">\n",
    "> Y normalized using one-hot encoding\n",
    "\n",
    "### PART 2 : Neural Network\n",
    "1.   NN Structure\n",
    "2.   Optimizer : Adam(learning_rate=0.0001)\n",
    "3.   Loss      : MAE (Mean Average Error)\n",
    "\n",
    "### PART 3 : Training\n",
    "1.   Training\n",
    "2.   Smoothing using Savitzky-Golay filter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4fC_XBf2xQ-"
   },
   "source": [
    "# PART 1 : Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88IHXBJFMufL",
    "outputId": "4525b056-b72b-4e6e-d766-e91bc8cfee95"
   },
   "outputs": [],
   "source": [
    "!pip install scikeras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edJESBph21is"
   },
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lwywzl6DDrXp"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMT_LXPvEXVd"
   },
   "source": [
    "### Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "akXNy_TDEUth",
    "outputId": "3e9e17e6-0a40-4b8b-f01c-084430d9f5a4"
   },
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "  print('Running on CoLab')\n",
    "  print('Downloding the data through internet...')\n",
    "  df = pd.read_csv(\"https://raw.githubusercontent.com/yokahealthcare/Anasa-GAN/master/dataset/breathing_waveform_data.csv\").iloc[:, :-1] # get rid of last column (\"notes\")\n",
    "else:\n",
    "  print('Not running on CoLab')\n",
    "  print('Use the dataset from folder')\n",
    "  df = pd.read_csv(\"dataset/breathing_waveform_data.csv\").iloc[:, :-1] # get rid of last column (\"notes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJHNMI1EEeNh"
   },
   "source": [
    "### Filter the zeros values\n",
    "> This will filtered the zeros value from all column (except first column)\n",
    ">\n",
    "> CAUSE : I think is natural for the first column to be 0.0 (because the time(X) still on 0 second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G3GgDl0dEYOt"
   },
   "outputs": [],
   "source": [
    "zeros_val = df[df.iloc[:, 1:].eq(0).any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YeZBn2KeEneR"
   },
   "source": [
    "### Drop the table that has value zeros on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sxdQlahIEYKT"
   },
   "outputs": [],
   "source": [
    "df = df[~df.isin(zeros_val)].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yslyj_2KFzxZ",
    "outputId": "fc47c120-e1ea-4ab4-b68f-f45dd6b13730"
   },
   "outputs": [],
   "source": [
    "df['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYs6rxwqJESP"
   },
   "source": [
    "### Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4malxQJ4SVo"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "def preprocessor(X, x_type=\"MinMaxScaler\"):\n",
    "  std_scaler = StandardScaler().fit(X)\n",
    "  min_max_scaler = MinMaxScaler().fit(X)\n",
    "\n",
    "  if x_type == \"StandardScaler\":\n",
    "    return std_scaler.transform(X)\n",
    "  elif x_type == \"MinMaxScaler\":\n",
    "    return min_max_scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ytoRT3LGmye"
   },
   "source": [
    "# PART 2 : Setup The Neural Network + Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DN95tLX5Gue"
   },
   "source": [
    "### Importing Neural Network Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdpxkFBm5GOO"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Activation\n",
    "\n",
    "from keras.optimizers import Adamax\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scikeras.wrappers import KerasRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DezqaGqwZFVe"
   },
   "source": [
    "### Neural Network : Deep Fully Connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDbiNLGFRR6k"
   },
   "outputs": [],
   "source": [
    "def AE(dropout_rate=0.2, init_mode='glorot_uniform'):\n",
    "    model = Sequential()\n",
    "    # Encoder layers\n",
    "    model.add(Dense(64, kernel_initializer=init_mode, input_shape=(85,)))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(32))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dense(16))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dense(8))\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    # Decoder layers\n",
    "    model.add(Dense(16, input_shape=(8,)))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dense(32))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dense(64))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dense(85))\n",
    "    model.add(Activation(tf.keras.activations.sigmoid))\n",
    "\n",
    "    model.compile(optimizer=Adamax(), loss='mae')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_lbAg1mK1pM"
   },
   "source": [
    "### GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cw9-7z23L39F",
    "outputId": "25aacd49-1a91-4cdf-b589-9b45a0f8f4c5"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "print(f\"Number of CPU cores: {cpu_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KncaWg958bi6"
   },
   "outputs": [],
   "source": [
    "def fit_and_print(tran_X, params, cv=2, n_jobs=cpu_count-2):\n",
    "  model_kr = KerasRegressor(model=AE)\n",
    "\n",
    "  # Create the grid search object\n",
    "  grid_search = GridSearchCV(estimator=model_kr, param_grid=params, cv=2, verbose=1, refit=True, n_jobs=n_jobs)\n",
    "\n",
    "  # Fit the grid search object to your data\n",
    "  grid_search.fit(tran_X, tran_X)  # Assuming X_train is your training data\n",
    "\n",
    "  # Print the best hyperparameters and the corresponding mean squared error\n",
    "  print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "  print(\"Best MSE:\", -grid_search.best_score_)\n",
    "\n",
    "  return grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uwXGOOGA_4R"
   },
   "source": [
    "# PART 3 : Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_lS5WtdBaMf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def save_all(tran_X, dir, grid_search):\n",
    "  # complement that\n",
    "  dir += '\\\\'\n",
    "\n",
    "  ## save the best model\n",
    "  # You have add or remove the argument according to GridSearchCV HYPERPARAMATERS\n",
    "  main_model = AE()\n",
    "  history = main_model.fit(tran_X, tran_X, epochs=10, batch_size=32, verbose=0)\n",
    "  main_model.save(dir+\"best_model.h5\")\n",
    "\n",
    "  # plot the loss on training the best model\n",
    "  plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "  plt.legend()\n",
    "  print(\"Mean Average Error : {}%\".format(np.mean(history.history[\"loss\"])*100))\n",
    "\n",
    "  ## save the result of gridsearchcv\n",
    "  res = pd.DataFrame(grid_search.cv_results_)\n",
    "  res.to_csv(dir+\"grid_search_result.csv\")\n",
    "\n",
    "  ## Create the encoder model using the trained weights\n",
    "  encoder_output = main_model.layers[8].output\n",
    "  encoder_model = tf.keras.Model(inputs=main_model.input, outputs=encoder_output)\n",
    "  encoder_model.save(dir+\"best_model_encoder.h5\")\n",
    "\n",
    "  ## Create the decoder model using the trained weights\n",
    "  decoder_input = main_model.layers[9].input\n",
    "  decoder_model = tf.keras.Model(inputs=decoder_input, outputs=main_model.output)\n",
    "  decoder_model.save(dir+\"best_model_decoder.h5\")\n",
    "\n",
    "  print(\"All Successfully Saved!\")\n",
    "  return res, main_model, encoder_model, decoder_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJ8KKzpRLaq4"
   },
   "source": [
    "# PART 4 : Testing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCtO12xs8f_1"
   },
   "source": [
    "### Define a function to smoothing the wave curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K2OKNU7KunWf"
   },
   "outputs": [],
   "source": [
    "# smoothing the wave of decoded_data\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "def smooth_wave(wave):\n",
    "  # Define the parameters for the Savitzky-Golay filter\n",
    "  window_length = 10  # The length of the window (odd number)\n",
    "  polyorder = 2  # The order of the polynomial fit\n",
    "\n",
    "  return savgol_filter(wave, window_length, polyorder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Uc6vII_TU0H"
   },
   "source": [
    "### Define a function to plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FskgEP2Uuv1c"
   },
   "outputs": [],
   "source": [
    "def plot_graph(tran_X, reconstructed, loss, num_samples=20):\n",
    "  row = int(num_samples / 5)\n",
    "\n",
    "  # Create figure and axis objects\n",
    "  fig, ax = plt.subplots(row, 5, figsize=(20,row*3))\n",
    "\n",
    "  idx=0\n",
    "  for y in range(row):\n",
    "    for x in range(5):\n",
    "      # Plot each time series\n",
    "      ax[y, x].plot(tran_X[idx], 'b')\n",
    "      ax[y, x].plot(reconstructed[idx], 'g')\n",
    "      ax[y, x].fill_between(np.arange(X.shape[1]), reconstructed[idx], tran_X[idx], color='lightcoral')\n",
    "      ax[y, x].set_title(\"Data {}; err : {:.2f}%\".format(idx, loss[idx]*100))\n",
    "      #ax[y, x].legend()\n",
    "\n",
    "      idx += 1\n",
    "\n",
    "  # legend\n",
    "  plt.legend(labels=[\"Input\", \"Reconstruction\", \"Error\"], loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "  # Customize the overall layout\n",
    "  plt.tight_layout()\n",
    "\n",
    "  # Show\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-g_DT1TLpyr"
   },
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WslvmNhXs3fo"
   },
   "outputs": [],
   "source": [
    "def test_model(tran_X, encoder_model, decoder_model):\n",
    "  # encode & decode the wave\n",
    "  print(\"Encoding...\")\n",
    "  encoded = encoder_model.predict(tran_X)\n",
    "  print(\"Decoding...\")\n",
    "  decoded = decoder_model.predict(encoded)\n",
    "\n",
    "  # Apply the Savitzky-Golay filter (smoothing the wave)\n",
    "  decoded_smoothed = smooth_wave(decoded)\n",
    "\n",
    "  # Calculate the Mean Average Error (MAE)\n",
    "  loss_actual = tf.keras.losses.mae(decoded, tran_X)\n",
    "  loss_smoothed = tf.keras.losses.mae(decoded_smoothed, tran_X)\n",
    "  print(\"[actual] Mean Average Error : {}%\".format(np.mean(loss_actual * 100)))\n",
    "  print(\"[smoothed] Mean Average Error : {}%\".format(np.mean(loss_smoothed * 100)))\n",
    "\n",
    "  # plot the reconstructed data vs actual data\n",
    "  plot_graph(tran_X, decoded_smoothed, loss_smoothed, num_samples=25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiWtZyZCwrPp"
   },
   "source": [
    "# PART 5 : ALL IN ONE BUTTON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J2_FpkNwpfgD"
   },
   "outputs": [],
   "source": [
    "def run_all(X):\n",
    "  # Preprocessed the data\n",
    "  tran_X = preprocessor(X, x_type=\"MinMaxScaler\")\n",
    "\n",
    "  # start the grid search cross validation process\n",
    "  # Define the hyperparameters and their values to search\n",
    "  param_grid = {\n",
    "      'epochs': [10],\n",
    "      'batch_size': [32],\n",
    "      'model__dropout_rate': [0.2],\n",
    "      'model__init_mode': ['glorot_uniform']\n",
    "  }\n",
    "\n",
    "  grid_search = fit_and_print(tran_X, params=param_grid, cv=2, n_jobs=-1)\n",
    "\n",
    "  # Saving the model\n",
    "  dir = os.getcwd()+\"\\\\\"+\"GAN Note\\\\[AE] AutoEncoder\" + \"\"\n",
    "  gs_csv, main_model, encoder_model, decoder_model = save_all(tran_X, dir, grid_search)\n",
    "\n",
    "  # Testing the model\n",
    "  test_model(tran_X, encoder_model, decoder_model)\n",
    "\n",
    "  return gs_csv, main_model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 6 : RUNNING THE CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DarVtTeFfd_"
   },
   "source": [
    "### Separate the data according to their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_4XvpfzEYBR"
   },
   "outputs": [],
   "source": [
    "normal_df = df[df['labels'] == \"normal\"]\n",
    "quick_df = df[df['labels'] == \"quick\"]\n",
    "hold_df = df[df['labels'] == \"hold\"]\n",
    "deep_df = df[df['labels'] == \"deep\"]\n",
    "deep_quick_df = df[df['labels'] == \"deep_quick\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9hk--KOJNNx"
   },
   "source": [
    "### Seperate the data (X)\n",
    "> we don't need Y (label) because we already seperate the dataset according to their label each one\n",
    ">\n",
    "> **\"normal\"** (19060), **\"quick\"** (2667), **\"hold\"** (2133), **\"deep\"** (1066), **\"deep_quick\"** (800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qi5q4DpoJLgr",
    "outputId": "79966541-d1a4-4649-997a-223a95c7d7ed"
   },
   "outputs": [],
   "source": [
    "# Seperate the X only\n",
    "X = normal_df.iloc[:, :-1]\n",
    "\n",
    "# Running All in One (training & testing & saving model file)\n",
    "# gs_csv, main_model, encoder_model, decoder_model = run_all(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install autokeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessed the data\n",
    "tran_X = preprocessor(X, x_type=\"MinMaxScaler\")\n",
    "\n",
    "# Autokeras - AUTOML\n",
    "from autokeras import StructuredDataRegressor\n",
    "\n",
    "# define the search\n",
    "search = StructuredDataRegressor(max_trials=15, loss='mean_absolute_error')\n",
    "# perform the search\n",
    "search.fit(x=tran_X, y=tran_X, verbose=1)\n",
    "# evaluate the model\n",
    "mae, _ = search.evaluate(X_test, y_test, verbose=0)\n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BH6mnrbz4jI"
   },
   "source": [
    "# PART 7 : Generating New Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edxDBOJ-xNxC"
   },
   "source": [
    "### Define a function to plotting the generated data result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yEVgCWga0GHF"
   },
   "outputs": [],
   "source": [
    "def plot_graph_generated(generated_data, num_samples=10, title=\"You can write the title\"):\n",
    "  # Create figure and axis objects\n",
    "  row = int(num_samples / 5)\n",
    "\n",
    "  fig, ax = plt.subplots(row, 5, figsize=(20,row*3))\n",
    "\n",
    "  idx=0\n",
    "  for y in range(row):\n",
    "    for x in range(5):\n",
    "      # Plot each time series\n",
    "      ax[y, x].plot(generated_data[idx], 'b')\n",
    "      ax[y, x].set_title(\"Data {}\".format(idx))\n",
    "\n",
    "      idx += 1\n",
    "\n",
    "  # legend\n",
    "  plt.legend(labels=[\"Generated\"], loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "  # title\n",
    "  plt.suptitle(title)\n",
    "\n",
    "  # Customize the overall layout\n",
    "  plt.tight_layout()\n",
    "\n",
    "  # Show\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PeFB36yMxSEZ"
   },
   "source": [
    "### Setting up parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ob4C1KuQPIOR",
    "outputId": "aa03b4b5-a1cd-4487-d0e9-dcfe3c5a33fc"
   },
   "outputs": [],
   "source": [
    "# Set the number of data points to generate\n",
    "generated_samples = 10\n",
    "# Randomly sample latent vectors from a predefined range\n",
    "noise_vectors = np.random.rand(*(generated_samples, 85))\n",
    "# Generate new data by decoding the latent vectors\n",
    "generated_data = main_model.predict(noise_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tt0IEwTBxYT6"
   },
   "source": [
    "### Plot the generated data before & after smoothing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "b7SGmf6_Yh1o",
    "outputId": "348695b8-5c6a-4a63-d098-e287f7cda0bc"
   },
   "outputs": [],
   "source": [
    "# Plot the graph before smoothing process\n",
    "plot_graph_generated(noise_vectors, num_samples=generated_samples, title=\"Noise\")\n",
    "# Plot the graph before smoothing process\n",
    "plot_graph_generated(generated_data, num_samples=generated_samples, title=\"Generated Data without Smoothing\")\n",
    "\n",
    "# Apply the Savitzky-Golay filter\n",
    "generated_data_smoothed = smooth_wave(generated_data)\n",
    "\n",
    "# Plot the graph after smoothing process\n",
    "plot_graph_generated(generated_data_smoothed, num_samples=generated_samples, title=\"Generated Data with Smoothing\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
