{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "013b3c3a",
   "metadata": {},
   "source": [
    "# BREATHING WAVE\n",
    "## ANN using PyTorch\n",
    "### 05 May 2023\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f3c609",
   "metadata": {},
   "source": [
    "## PART 1 : Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c80ceec",
   "metadata": {},
   "source": [
    "### Importing Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a98a04f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bc3fa9",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcd4c5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset/breathing_waveform_data.csv\").iloc[:, :-1] # get rid of last column (\"notes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef0607f",
   "metadata": {},
   "source": [
    "### Filter the zeros values\n",
    "> This will filtered the zeros value from all column (except first column)\n",
    ">\n",
    "> CAUSE : I think is natural for the first column to be 0.0 (because the time(X) still on 0 second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35bc6bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros_val = df[df.iloc[:, 1:].eq(0).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fa4affd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5473</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5474</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5475</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5476</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5477</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6142</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6143</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6144</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6145</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6146</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>674 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0    1    2    3    4    5    6    7    8    9  ...   76   77   78  \\\n",
       "5473  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "5474  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "5475  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "5476  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "5477  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "6142  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "6143  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "6144  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "6145  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "6146  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "       79   80   81   82   83   84  labels  \n",
       "5473  0.0  0.0  0.0  0.0  0.0  0.0  normal  \n",
       "5474  0.0  0.0  0.0  0.0  0.0  0.0  normal  \n",
       "5475  0.0  0.0  0.0  0.0  0.0  0.0  normal  \n",
       "5476  0.0  0.0  0.0  0.0  0.0  0.0  normal  \n",
       "5477  0.0  0.0  0.0  0.0  0.0  0.0  normal  \n",
       "...   ...  ...  ...  ...  ...  ...     ...  \n",
       "6142  0.0  0.0  0.0  0.0  0.0  0.0  normal  \n",
       "6143  0.0  0.0  0.0  0.0  0.0  0.0  normal  \n",
       "6144  0.0  0.0  0.0  0.0  0.0  0.0  normal  \n",
       "6145  0.0  0.0  0.0  0.0  0.0  0.0  normal  \n",
       "6146  0.0  0.0  0.0  0.0  0.0  0.0  normal  \n",
       "\n",
       "[674 rows x 86 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6cf429",
   "metadata": {},
   "source": [
    "### Drop the table that has value zeros on it\n",
    "> Size of original data : 26400 rows\n",
    ">\n",
    "> Size of zero values : 674 rows\n",
    ">\n",
    "> Result : 26400 - 674 = 25726 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "578d9b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.isin(zeros_val)].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fed4fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.483309</td>\n",
       "      <td>0.459790</td>\n",
       "      <td>0.431024</td>\n",
       "      <td>0.376565</td>\n",
       "      <td>0.295734</td>\n",
       "      <td>0.193290</td>\n",
       "      <td>0.066060</td>\n",
       "      <td>-0.083445</td>\n",
       "      <td>-0.247221</td>\n",
       "      <td>-0.409374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.391514</td>\n",
       "      <td>0.452677</td>\n",
       "      <td>0.521407</td>\n",
       "      <td>0.595845</td>\n",
       "      <td>0.661691</td>\n",
       "      <td>0.702932</td>\n",
       "      <td>0.708613</td>\n",
       "      <td>0.682564</td>\n",
       "      <td>0.637765</td>\n",
       "      <td>deep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.044518</td>\n",
       "      <td>-1.935588</td>\n",
       "      <td>-1.808629</td>\n",
       "      <td>-1.667919</td>\n",
       "      <td>-1.513497</td>\n",
       "      <td>-1.348760</td>\n",
       "      <td>-1.171044</td>\n",
       "      <td>-0.972509</td>\n",
       "      <td>-0.759554</td>\n",
       "      <td>-0.547793</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138731</td>\n",
       "      <td>-0.053860</td>\n",
       "      <td>-0.241691</td>\n",
       "      <td>-0.417603</td>\n",
       "      <td>-0.582320</td>\n",
       "      <td>-0.738485</td>\n",
       "      <td>-0.889731</td>\n",
       "      <td>-1.037066</td>\n",
       "      <td>-1.174654</td>\n",
       "      <td>deep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.213535</td>\n",
       "      <td>-1.269056</td>\n",
       "      <td>-1.323306</td>\n",
       "      <td>-1.375251</td>\n",
       "      <td>-1.430062</td>\n",
       "      <td>-1.485479</td>\n",
       "      <td>-1.529200</td>\n",
       "      <td>-1.557172</td>\n",
       "      <td>-1.574662</td>\n",
       "      <td>-1.575457</td>\n",
       "      <td>...</td>\n",
       "      <td>0.947940</td>\n",
       "      <td>0.996154</td>\n",
       "      <td>1.035743</td>\n",
       "      <td>1.049543</td>\n",
       "      <td>1.024204</td>\n",
       "      <td>0.954716</td>\n",
       "      <td>0.844505</td>\n",
       "      <td>0.702445</td>\n",
       "      <td>0.541555</td>\n",
       "      <td>deep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.914806</td>\n",
       "      <td>-0.887726</td>\n",
       "      <td>-0.856065</td>\n",
       "      <td>-0.823527</td>\n",
       "      <td>-0.794551</td>\n",
       "      <td>-0.768074</td>\n",
       "      <td>-0.740895</td>\n",
       "      <td>-0.713364</td>\n",
       "      <td>-0.685445</td>\n",
       "      <td>-0.652020</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.478218</td>\n",
       "      <td>-0.571465</td>\n",
       "      <td>-0.684115</td>\n",
       "      <td>-0.817078</td>\n",
       "      <td>-0.966231</td>\n",
       "      <td>-1.122537</td>\n",
       "      <td>-1.264759</td>\n",
       "      <td>-1.376908</td>\n",
       "      <td>-1.461059</td>\n",
       "      <td>deep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.547469</td>\n",
       "      <td>-1.458818</td>\n",
       "      <td>-1.362120</td>\n",
       "      <td>-1.264829</td>\n",
       "      <td>-1.164948</td>\n",
       "      <td>-1.060064</td>\n",
       "      <td>-0.954496</td>\n",
       "      <td>-0.849448</td>\n",
       "      <td>-0.742812</td>\n",
       "      <td>-0.636614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227050</td>\n",
       "      <td>0.130983</td>\n",
       "      <td>0.041438</td>\n",
       "      <td>-0.038034</td>\n",
       "      <td>-0.106152</td>\n",
       "      <td>-0.163048</td>\n",
       "      <td>-0.210926</td>\n",
       "      <td>-0.253102</td>\n",
       "      <td>-0.290270</td>\n",
       "      <td>deep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26395</th>\n",
       "      <td>-0.152463</td>\n",
       "      <td>-0.164723</td>\n",
       "      <td>-0.165409</td>\n",
       "      <td>-0.152623</td>\n",
       "      <td>-0.118115</td>\n",
       "      <td>-0.066218</td>\n",
       "      <td>-0.010253</td>\n",
       "      <td>0.041637</td>\n",
       "      <td>0.092217</td>\n",
       "      <td>0.140510</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.336787</td>\n",
       "      <td>-0.306774</td>\n",
       "      <td>-0.280607</td>\n",
       "      <td>-0.269843</td>\n",
       "      <td>-0.260062</td>\n",
       "      <td>-0.229981</td>\n",
       "      <td>-0.167654</td>\n",
       "      <td>-0.082300</td>\n",
       "      <td>0.004372</td>\n",
       "      <td>quick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26396</th>\n",
       "      <td>-0.164723</td>\n",
       "      <td>-0.165409</td>\n",
       "      <td>-0.152623</td>\n",
       "      <td>-0.118115</td>\n",
       "      <td>-0.066218</td>\n",
       "      <td>-0.010253</td>\n",
       "      <td>0.041637</td>\n",
       "      <td>0.092217</td>\n",
       "      <td>0.140510</td>\n",
       "      <td>0.188025</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.306774</td>\n",
       "      <td>-0.280607</td>\n",
       "      <td>-0.269843</td>\n",
       "      <td>-0.260062</td>\n",
       "      <td>-0.229981</td>\n",
       "      <td>-0.167654</td>\n",
       "      <td>-0.082300</td>\n",
       "      <td>0.004372</td>\n",
       "      <td>0.089958</td>\n",
       "      <td>quick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26397</th>\n",
       "      <td>-0.165409</td>\n",
       "      <td>-0.152623</td>\n",
       "      <td>-0.118115</td>\n",
       "      <td>-0.066218</td>\n",
       "      <td>-0.010253</td>\n",
       "      <td>0.041637</td>\n",
       "      <td>0.092217</td>\n",
       "      <td>0.140510</td>\n",
       "      <td>0.188025</td>\n",
       "      <td>0.240939</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.280607</td>\n",
       "      <td>-0.269843</td>\n",
       "      <td>-0.260062</td>\n",
       "      <td>-0.229981</td>\n",
       "      <td>-0.167654</td>\n",
       "      <td>-0.082300</td>\n",
       "      <td>0.004372</td>\n",
       "      <td>0.089958</td>\n",
       "      <td>0.179209</td>\n",
       "      <td>quick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26398</th>\n",
       "      <td>-0.152623</td>\n",
       "      <td>-0.118115</td>\n",
       "      <td>-0.066218</td>\n",
       "      <td>-0.010253</td>\n",
       "      <td>0.041637</td>\n",
       "      <td>0.092217</td>\n",
       "      <td>0.140510</td>\n",
       "      <td>0.188025</td>\n",
       "      <td>0.240939</td>\n",
       "      <td>0.294399</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.269843</td>\n",
       "      <td>-0.260062</td>\n",
       "      <td>-0.229981</td>\n",
       "      <td>-0.167654</td>\n",
       "      <td>-0.082300</td>\n",
       "      <td>0.004372</td>\n",
       "      <td>0.089958</td>\n",
       "      <td>0.179209</td>\n",
       "      <td>0.264014</td>\n",
       "      <td>quick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26399</th>\n",
       "      <td>-0.118115</td>\n",
       "      <td>-0.066218</td>\n",
       "      <td>-0.010253</td>\n",
       "      <td>0.041637</td>\n",
       "      <td>0.092217</td>\n",
       "      <td>0.140510</td>\n",
       "      <td>0.188025</td>\n",
       "      <td>0.240939</td>\n",
       "      <td>0.294399</td>\n",
       "      <td>0.340346</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.260062</td>\n",
       "      <td>-0.229981</td>\n",
       "      <td>-0.167654</td>\n",
       "      <td>-0.082300</td>\n",
       "      <td>0.004372</td>\n",
       "      <td>0.089958</td>\n",
       "      <td>0.179209</td>\n",
       "      <td>0.264014</td>\n",
       "      <td>0.343418</td>\n",
       "      <td>quick</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25726 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0      0.483309  0.459790  0.431024  0.376565  0.295734  0.193290  0.066060   \n",
       "1     -2.044518 -1.935588 -1.808629 -1.667919 -1.513497 -1.348760 -1.171044   \n",
       "2     -1.213535 -1.269056 -1.323306 -1.375251 -1.430062 -1.485479 -1.529200   \n",
       "3     -0.914806 -0.887726 -0.856065 -0.823527 -0.794551 -0.768074 -0.740895   \n",
       "4     -1.547469 -1.458818 -1.362120 -1.264829 -1.164948 -1.060064 -0.954496   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "26395 -0.152463 -0.164723 -0.165409 -0.152623 -0.118115 -0.066218 -0.010253   \n",
       "26396 -0.164723 -0.165409 -0.152623 -0.118115 -0.066218 -0.010253  0.041637   \n",
       "26397 -0.165409 -0.152623 -0.118115 -0.066218 -0.010253  0.041637  0.092217   \n",
       "26398 -0.152623 -0.118115 -0.066218 -0.010253  0.041637  0.092217  0.140510   \n",
       "26399 -0.118115 -0.066218 -0.010253  0.041637  0.092217  0.140510  0.188025   \n",
       "\n",
       "              7         8         9  ...        76        77        78  \\\n",
       "0     -0.083445 -0.247221 -0.409374  ...  0.391514  0.452677  0.521407   \n",
       "1     -0.972509 -0.759554 -0.547793  ...  0.138731 -0.053860 -0.241691   \n",
       "2     -1.557172 -1.574662 -1.575457  ...  0.947940  0.996154  1.035743   \n",
       "3     -0.713364 -0.685445 -0.652020  ... -0.478218 -0.571465 -0.684115   \n",
       "4     -0.849448 -0.742812 -0.636614  ...  0.227050  0.130983  0.041438   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "26395  0.041637  0.092217  0.140510  ... -0.336787 -0.306774 -0.280607   \n",
       "26396  0.092217  0.140510  0.188025  ... -0.306774 -0.280607 -0.269843   \n",
       "26397  0.140510  0.188025  0.240939  ... -0.280607 -0.269843 -0.260062   \n",
       "26398  0.188025  0.240939  0.294399  ... -0.269843 -0.260062 -0.229981   \n",
       "26399  0.240939  0.294399  0.340346  ... -0.260062 -0.229981 -0.167654   \n",
       "\n",
       "             79        80        81        82        83        84  labels  \n",
       "0      0.595845  0.661691  0.702932  0.708613  0.682564  0.637765    deep  \n",
       "1     -0.417603 -0.582320 -0.738485 -0.889731 -1.037066 -1.174654    deep  \n",
       "2      1.049543  1.024204  0.954716  0.844505  0.702445  0.541555    deep  \n",
       "3     -0.817078 -0.966231 -1.122537 -1.264759 -1.376908 -1.461059    deep  \n",
       "4     -0.038034 -0.106152 -0.163048 -0.210926 -0.253102 -0.290270    deep  \n",
       "...         ...       ...       ...       ...       ...       ...     ...  \n",
       "26395 -0.269843 -0.260062 -0.229981 -0.167654 -0.082300  0.004372   quick  \n",
       "26396 -0.260062 -0.229981 -0.167654 -0.082300  0.004372  0.089958   quick  \n",
       "26397 -0.229981 -0.167654 -0.082300  0.004372  0.089958  0.179209   quick  \n",
       "26398 -0.167654 -0.082300  0.004372  0.089958  0.179209  0.264014   quick  \n",
       "26399 -0.082300  0.004372  0.089958  0.179209  0.264014  0.343418   quick  \n",
       "\n",
       "[25726 rows x 86 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5e6aca",
   "metadata": {},
   "source": [
    "### Separate the X (data) and Y (label) & check for the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eba61b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X have a null? \tFalse\n",
      "Y have a null? \tFalse\n"
     ]
    }
   ],
   "source": [
    "X = df.iloc[:, :-1]\n",
    "Y = df.iloc[:, -1]\n",
    "\n",
    "# Check if the data do not have any NULL \n",
    "print(\"X have a null? \\t{}\".format(X.isnull().values.any()))\n",
    "print(\"Y have a null? \\t{}\".format(Y.isnull().values.any()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1736c0f",
   "metadata": {},
   "source": [
    "### Scalling the data (X) using StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ee32797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cd27827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.57732521,  1.50096158,  1.40710808, ...,  2.29629039,\n",
       "         2.21721275,  2.07692465],\n",
       "       [-6.66012793, -6.30517583, -5.88996555, ..., -2.8575958 ,\n",
       "        -3.34130049, -3.79623272],\n",
       "       [-3.95219545, -4.13306073, -4.30872165, ...,  2.73447696,\n",
       "         2.28147373,  1.76515392],\n",
       "       ...,\n",
       "       [-0.53665953, -0.49479087, -0.38205583, ...,  0.02545401,\n",
       "         0.30168015,  0.59097159],\n",
       "       [-0.49499252, -0.38233365, -0.21296997, ...,  0.30142842,\n",
       "         0.5901747 ,  0.8657814 ],\n",
       "       [-0.38253957, -0.21321113, -0.03062927, ...,  0.58922045,\n",
       "         0.86429589,  1.12308954]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c5ef4d",
   "metadata": {},
   "source": [
    "### Transform the labe (Y) to one hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "272742fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# encode class values as integers [0,0,0,0,0,0,0,1,1,1,1,1,2,2,2,2]\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "hot_y = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "331d28cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hot_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16f7e46",
   "metadata": {},
   "source": [
    "### Split the data (X) and label (Y) using train_test_split()\n",
    "> I will set the split ratio 80% training and 20% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25ac0e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, hot_y, test_size=.2, random_state=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a945996e",
   "metadata": {},
   "source": [
    "## PART 2 : Building Neural Network using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05cd94d",
   "metadata": {},
   "source": [
    "### Importing pytorch libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "368d1453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c560ad9f",
   "metadata": {},
   "source": [
    "### Convert all the training and testing data into PyTorch Tensor type variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26918b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.Tensor(X_train)\n",
    "X_test = torch.Tensor(X_test)\n",
    "Y_train = torch.Tensor(Y_train)\n",
    "Y_test = torch.Tensor(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbf64e4",
   "metadata": {},
   "source": [
    "### Define the model acrhitecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9dc82b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8796e5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train.shape[1]\n",
    "output_size = Y_train.shape[1]\n",
    "model = MyModel(input_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bcb014",
   "metadata": {},
   "source": [
    "### Initiliaze the loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ccc44d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aa9db7",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c66b3b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 1.5349531173706055\n",
      "Epoch 2/200, Loss: 1.5067249536514282\n",
      "Epoch 3/200, Loss: 1.2525296211242676\n",
      "Epoch 4/200, Loss: 1.2125089168548584\n",
      "Epoch 5/200, Loss: 1.1518081426620483\n",
      "Epoch 6/200, Loss: 1.0826128721237183\n",
      "Epoch 7/200, Loss: 1.0155946016311646\n",
      "Epoch 8/200, Loss: 0.9280393719673157\n",
      "Epoch 9/200, Loss: 0.8735841512680054\n",
      "Epoch 10/200, Loss: 0.8301234841346741\n",
      "Epoch 11/200, Loss: 0.80362468957901\n",
      "Epoch 12/200, Loss: 0.8266608119010925\n",
      "Epoch 13/200, Loss: 0.7976644039154053\n",
      "Epoch 14/200, Loss: 0.7406864762306213\n",
      "Epoch 15/200, Loss: 0.7912884950637817\n",
      "Epoch 16/200, Loss: 0.7277735471725464\n",
      "Epoch 17/200, Loss: 0.7363976240158081\n",
      "Epoch 18/200, Loss: 0.7170231342315674\n",
      "Epoch 19/200, Loss: 0.715427577495575\n",
      "Epoch 20/200, Loss: 0.6949171423912048\n",
      "Epoch 21/200, Loss: 0.6888406276702881\n",
      "Epoch 22/200, Loss: 0.6837934255599976\n",
      "Epoch 23/200, Loss: 0.6703577637672424\n",
      "Epoch 24/200, Loss: 0.6869359612464905\n",
      "Epoch 25/200, Loss: 0.6608166098594666\n",
      "Epoch 26/200, Loss: 0.6673275828361511\n",
      "Epoch 27/200, Loss: 0.6447646021842957\n",
      "Epoch 28/200, Loss: 0.6506240367889404\n",
      "Epoch 29/200, Loss: 0.6415881514549255\n",
      "Epoch 30/200, Loss: 0.6411663293838501\n",
      "Epoch 31/200, Loss: 0.628742516040802\n",
      "Epoch 32/200, Loss: 0.6284841299057007\n",
      "Epoch 33/200, Loss: 0.6232394576072693\n",
      "Epoch 34/200, Loss: 0.6180737018585205\n",
      "Epoch 35/200, Loss: 0.6154731512069702\n",
      "Epoch 36/200, Loss: 0.6092404127120972\n",
      "Epoch 37/200, Loss: 0.6070424318313599\n",
      "Epoch 38/200, Loss: 0.597994327545166\n",
      "Epoch 39/200, Loss: 0.59574955701828\n",
      "Epoch 40/200, Loss: 0.5916218757629395\n",
      "Epoch 41/200, Loss: 0.5858445167541504\n",
      "Epoch 42/200, Loss: 0.5809442400932312\n",
      "Epoch 43/200, Loss: 0.5756385326385498\n",
      "Epoch 44/200, Loss: 0.5702567100524902\n",
      "Epoch 45/200, Loss: 0.5650263428688049\n",
      "Epoch 46/200, Loss: 0.5612436532974243\n",
      "Epoch 47/200, Loss: 0.554026186466217\n",
      "Epoch 48/200, Loss: 0.5579138398170471\n",
      "Epoch 49/200, Loss: 0.5871461033821106\n",
      "Epoch 50/200, Loss: 0.6092132925987244\n",
      "Epoch 51/200, Loss: 0.5733461976051331\n",
      "Epoch 52/200, Loss: 0.55206698179245\n",
      "Epoch 53/200, Loss: 0.5727826356887817\n",
      "Epoch 54/200, Loss: 0.5241104364395142\n",
      "Epoch 55/200, Loss: 0.5508589744567871\n",
      "Epoch 56/200, Loss: 0.5176888108253479\n",
      "Epoch 57/200, Loss: 0.5365811586380005\n",
      "Epoch 58/200, Loss: 0.5052757859230042\n",
      "Epoch 59/200, Loss: 0.5217952132225037\n",
      "Epoch 60/200, Loss: 0.49626001715660095\n",
      "Epoch 61/200, Loss: 0.5089539289474487\n",
      "Epoch 62/200, Loss: 0.4834728538990021\n",
      "Epoch 63/200, Loss: 0.4959411323070526\n",
      "Epoch 64/200, Loss: 0.47549501061439514\n",
      "Epoch 65/200, Loss: 0.48617517948150635\n",
      "Epoch 66/200, Loss: 0.46912243962287903\n",
      "Epoch 67/200, Loss: 0.47221213579177856\n",
      "Epoch 68/200, Loss: 0.4556075930595398\n",
      "Epoch 69/200, Loss: 0.4604153037071228\n",
      "Epoch 70/200, Loss: 0.4468798339366913\n",
      "Epoch 71/200, Loss: 0.44743645191192627\n",
      "Epoch 72/200, Loss: 0.43817999958992004\n",
      "Epoch 73/200, Loss: 0.43642351031303406\n",
      "Epoch 74/200, Loss: 0.4373888671398163\n",
      "Epoch 75/200, Loss: 0.4343099892139435\n",
      "Epoch 76/200, Loss: 0.4513280987739563\n",
      "Epoch 77/200, Loss: 0.4451683461666107\n",
      "Epoch 78/200, Loss: 0.4226026237010956\n",
      "Epoch 79/200, Loss: 0.41855961084365845\n",
      "Epoch 80/200, Loss: 0.4231666922569275\n",
      "Epoch 81/200, Loss: 0.41809237003326416\n",
      "Epoch 82/200, Loss: 0.3974939286708832\n",
      "Epoch 83/200, Loss: 0.4052533507347107\n",
      "Epoch 84/200, Loss: 0.3949507176876068\n",
      "Epoch 85/200, Loss: 0.39161020517349243\n",
      "Epoch 86/200, Loss: 0.37794625759124756\n",
      "Epoch 87/200, Loss: 0.3880165219306946\n",
      "Epoch 88/200, Loss: 0.37825027108192444\n",
      "Epoch 89/200, Loss: 0.36724525690078735\n",
      "Epoch 90/200, Loss: 0.370979905128479\n",
      "Epoch 91/200, Loss: 0.3682023882865906\n",
      "Epoch 92/200, Loss: 0.3542768955230713\n",
      "Epoch 93/200, Loss: 0.35874977707862854\n",
      "Epoch 94/200, Loss: 0.346408486366272\n",
      "Epoch 95/200, Loss: 0.3504028618335724\n",
      "Epoch 96/200, Loss: 0.34986671805381775\n",
      "Epoch 97/200, Loss: 0.34126386046409607\n",
      "Epoch 98/200, Loss: 0.32714954018592834\n",
      "Epoch 99/200, Loss: 0.3323683440685272\n",
      "Epoch 100/200, Loss: 0.3240090608596802\n",
      "Epoch 101/200, Loss: 0.31927838921546936\n",
      "Epoch 102/200, Loss: 0.31457221508026123\n",
      "Epoch 103/200, Loss: 0.31201860308647156\n",
      "Epoch 104/200, Loss: 0.30327141284942627\n",
      "Epoch 105/200, Loss: 0.30585306882858276\n",
      "Epoch 106/200, Loss: 0.2936205565929413\n",
      "Epoch 107/200, Loss: 0.29331502318382263\n",
      "Epoch 108/200, Loss: 0.29308855533599854\n",
      "Epoch 109/200, Loss: 0.30087143182754517\n",
      "Epoch 110/200, Loss: 0.32125404477119446\n",
      "Epoch 111/200, Loss: 0.3806609809398651\n",
      "Epoch 112/200, Loss: 0.39382800459861755\n",
      "Epoch 113/200, Loss: 0.3086511194705963\n",
      "Epoch 114/200, Loss: 0.32897815108299255\n",
      "Epoch 115/200, Loss: 0.32656753063201904\n",
      "Epoch 116/200, Loss: 0.3068617880344391\n",
      "Epoch 117/200, Loss: 0.317183256149292\n",
      "Epoch 118/200, Loss: 0.30579063296318054\n",
      "Epoch 119/200, Loss: 0.3150980472564697\n",
      "Epoch 120/200, Loss: 0.2847176194190979\n",
      "Epoch 121/200, Loss: 0.2880599796772003\n",
      "Epoch 122/200, Loss: 0.28841108083724976\n",
      "Epoch 123/200, Loss: 0.26917847990989685\n",
      "Epoch 124/200, Loss: 0.27614346146583557\n",
      "Epoch 125/200, Loss: 0.26193782687187195\n",
      "Epoch 126/200, Loss: 0.2616507112979889\n",
      "Epoch 127/200, Loss: 0.2588047385215759\n",
      "Epoch 128/200, Loss: 0.249974325299263\n",
      "Epoch 129/200, Loss: 0.2498127818107605\n",
      "Epoch 130/200, Loss: 0.24383768439292908\n",
      "Epoch 131/200, Loss: 0.24126318097114563\n",
      "Epoch 132/200, Loss: 0.23741421103477478\n",
      "Epoch 133/200, Loss: 0.23573122918605804\n",
      "Epoch 134/200, Loss: 0.22887346148490906\n",
      "Epoch 135/200, Loss: 0.2311118245124817\n",
      "Epoch 136/200, Loss: 0.22276617586612701\n",
      "Epoch 137/200, Loss: 0.2257370501756668\n",
      "Epoch 138/200, Loss: 0.21768514811992645\n",
      "Epoch 139/200, Loss: 0.21986529231071472\n",
      "Epoch 140/200, Loss: 0.2145778238773346\n",
      "Epoch 141/200, Loss: 0.21192939579486847\n",
      "Epoch 142/200, Loss: 0.21154801547527313\n",
      "Epoch 143/200, Loss: 0.20723673701286316\n",
      "Epoch 144/200, Loss: 0.2066577523946762\n",
      "Epoch 145/200, Loss: 0.20359773933887482\n",
      "Epoch 146/200, Loss: 0.2003335803747177\n",
      "Epoch 147/200, Loss: 0.20041592419147491\n",
      "Epoch 148/200, Loss: 0.19604086875915527\n",
      "Epoch 149/200, Loss: 0.19586163759231567\n",
      "Epoch 150/200, Loss: 0.19234302639961243\n",
      "Epoch 151/200, Loss: 0.191236212849617\n",
      "Epoch 152/200, Loss: 0.18921659886837006\n",
      "Epoch 153/200, Loss: 0.1873299926519394\n",
      "Epoch 154/200, Loss: 0.18538066744804382\n",
      "Epoch 155/200, Loss: 0.18341350555419922\n",
      "Epoch 156/200, Loss: 0.18120361864566803\n",
      "Epoch 157/200, Loss: 0.180026113986969\n",
      "Epoch 158/200, Loss: 0.17781028151512146\n",
      "Epoch 159/200, Loss: 0.17650677263736725\n",
      "Epoch 160/200, Loss: 0.17503727972507477\n",
      "Epoch 161/200, Loss: 0.17356671392917633\n",
      "Epoch 162/200, Loss: 0.1724708080291748\n",
      "Epoch 163/200, Loss: 0.17131945490837097\n",
      "Epoch 164/200, Loss: 0.1713770180940628\n",
      "Epoch 165/200, Loss: 0.1721845120191574\n",
      "Epoch 166/200, Loss: 0.1790996640920639\n",
      "Epoch 167/200, Loss: 0.19952382147312164\n",
      "Epoch 168/200, Loss: 0.2895863354206085\n",
      "Epoch 169/200, Loss: 0.4080941677093506\n",
      "Epoch 170/200, Loss: 0.46267518401145935\n",
      "Epoch 171/200, Loss: 0.33913454413414\n",
      "Epoch 172/200, Loss: 0.31717509031295776\n",
      "Epoch 173/200, Loss: 0.3008849322795868\n",
      "Epoch 174/200, Loss: 0.27360469102859497\n",
      "Epoch 175/200, Loss: 0.27554091811180115\n",
      "Epoch 176/200, Loss: 0.2498040646314621\n",
      "Epoch 177/200, Loss: 0.23719392716884613\n",
      "Epoch 178/200, Loss: 0.2520824670791626\n",
      "Epoch 179/200, Loss: 0.21246929466724396\n",
      "Epoch 180/200, Loss: 0.2352335900068283\n",
      "Epoch 181/200, Loss: 0.2099751979112625\n",
      "Epoch 182/200, Loss: 0.2058301419019699\n",
      "Epoch 183/200, Loss: 0.2029789835214615\n",
      "Epoch 184/200, Loss: 0.1963024139404297\n",
      "Epoch 185/200, Loss: 0.1927441656589508\n",
      "Epoch 186/200, Loss: 0.18070505559444427\n",
      "Epoch 187/200, Loss: 0.19153892993927002\n",
      "Epoch 188/200, Loss: 0.17606358230113983\n",
      "Epoch 189/200, Loss: 0.17160004377365112\n",
      "Epoch 190/200, Loss: 0.17586691677570343\n",
      "Epoch 191/200, Loss: 0.16695035994052887\n",
      "Epoch 192/200, Loss: 0.16516922414302826\n",
      "Epoch 193/200, Loss: 0.16579613089561462\n",
      "Epoch 194/200, Loss: 0.15889745950698853\n",
      "Epoch 195/200, Loss: 0.1575995534658432\n",
      "Epoch 196/200, Loss: 0.15725870430469513\n",
      "Epoch 197/200, Loss: 0.1531030386686325\n",
      "Epoch 198/200, Loss: 0.15095505118370056\n",
      "Epoch 199/200, Loss: 0.14957866072654724\n",
      "Epoch 200/200, Loss: 0.1488199681043625\n"
     ]
    }
   ],
   "source": [
    "num_of_epochs = 200\n",
    "for epoch in range(num_of_epochs):\n",
    "    # reset optimizer\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # forward pass\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, Y_train)\n",
    "    \n",
    "    # backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print the loss for monitoring the training progress\n",
    "    print(f\"Epoch {epoch+1}/{num_of_epochs}, Loss: {loss.item()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b852726",
   "metadata": {},
   "source": [
    "## PART 3 : Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd490a49",
   "metadata": {},
   "source": [
    "### Counting the average loss from testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2653ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.22212223124600483\n"
     ]
    }
   ],
   "source": [
    "# set the model to evaluation mode (e.g disable operation like dropout)\n",
    "model.eval()\n",
    "\n",
    "total_loss = 0\n",
    "\n",
    "for inputs, labels in zip(X_test, Y_test):\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # Calculate loss (if needed)\n",
    "    loss = criterion(outputs, labels)\n",
    "    total_loss += loss.item()\n",
    "\n",
    "# Calculate average loss\n",
    "average_loss = total_loss / X_test.shape[0]\n",
    "\n",
    "# Calculate average evaluation metrics (if needed)\n",
    "# ...\n",
    "\n",
    "# Print or store the evaluation results\n",
    "print(f\"Average Loss: {average_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b3d141",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a94e17",
   "metadata": {},
   "source": [
    "### It's good, very fast framework"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
