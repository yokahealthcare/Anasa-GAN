{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/yokahealthcare/Anasa-GAN/blob/master/%5BMain%20v2%5D%20%5BAE%5D%20%5BANN%5D%20Breathing_Wave.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mglGdAa9EXAw"
   },
   "source": [
    "# AE - AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bkqs7_K0UFuK"
   },
   "source": [
    "## Project Strucuture\n",
    "\n",
    "### PART 1 : Data Preprocessing\n",
    "\n",
    "\n",
    "1.   Filter the zeros values (except if in the first column)\n",
    "2.   Separate the data(q) according to labels\n",
    "3.   Seperate q into data(X) and label(Y)\n",
    "4.   Normalize the data\n",
    "> X normalized using MinMaxScaler between 0 and 1\n",
    ">\n",
    "> Y normalized using one-hot encoding\n",
    "\n",
    "### PART 2 : Neural Network\n",
    "1.   NN Structure\n",
    "2.   Optimizer : Adam(learning_rate=0.0001)\n",
    "3.   Loss      : MAE (Mean Average Error)\n",
    "\n",
    "### PART 3 : Training\n",
    "1.   Training\n",
    "2.   Smoothing using Savitzky-Golay filter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4fC_XBf2xQ-"
   },
   "source": [
    "## PART 1 : Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edJESBph21is"
   },
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Lwywzl6DDrXp"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMT_LXPvEXVd"
   },
   "source": [
    "### Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "akXNy_TDEUth"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/yokahealthcare/Anasa-GAN/master/dataset/breathing_waveform_data.csv\").iloc[:, :-1] # get rid of last column (\"notes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJHNMI1EEeNh"
   },
   "source": [
    "### Filter the zeros values\n",
    "> This will filtered the zeros value from all column (except first column)\n",
    ">\n",
    "> CAUSE : I think is natural for the first column to be 0.0 (because the time(X) still on 0 second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "G3GgDl0dEYOt"
   },
   "outputs": [],
   "source": [
    "zeros_val = df[df.iloc[:, 1:].eq(0).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "eIsb07EEEYMi",
    "outputId": "13045595-19ff-4316-f929-8cc70b7b2bb9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5473</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5474</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5475</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5476</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5477</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6142</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6143</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6144</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6145</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6146</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>674 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0    1    2    3    4    5    6    7    8    9  ...   76   77   78  \\\n",
       "5473  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "5474  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "5475  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "5476  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "5477  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "6142  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "6143  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "6144  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "6145  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "6146  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "       79   80   81   82   83   84  labels  \n",
       "5473  0.0  0.0  0.0  0.0  0.0  0.0  normal  \n",
       "5474  0.0  0.0  0.0  0.0  0.0  0.0  normal  \n",
       "5475  0.0  0.0  0.0  0.0  0.0  0.0  normal  \n",
       "5476  0.0  0.0  0.0  0.0  0.0  0.0  normal  \n",
       "5477  0.0  0.0  0.0  0.0  0.0  0.0  normal  \n",
       "...   ...  ...  ...  ...  ...  ...     ...  \n",
       "6142  0.0  0.0  0.0  0.0  0.0  0.0  normal  \n",
       "6143  0.0  0.0  0.0  0.0  0.0  0.0  normal  \n",
       "6144  0.0  0.0  0.0  0.0  0.0  0.0  normal  \n",
       "6145  0.0  0.0  0.0  0.0  0.0  0.0  normal  \n",
       "6146  0.0  0.0  0.0  0.0  0.0  0.0  normal  \n",
       "\n",
       "[674 rows x 86 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YeZBn2KeEneR"
   },
   "source": [
    "### Drop the table that has value zeros on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sxdQlahIEYKT"
   },
   "outputs": [],
   "source": [
    "df = df[~df.isin(zeros_val)].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "_kSRDlSgEYG_",
    "outputId": "5d6d18c9-c051-48ba-8abc-2b199777248e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.483309</td>\n",
       "      <td>0.459790</td>\n",
       "      <td>0.431024</td>\n",
       "      <td>0.376565</td>\n",
       "      <td>0.295734</td>\n",
       "      <td>0.193290</td>\n",
       "      <td>0.066060</td>\n",
       "      <td>-0.083445</td>\n",
       "      <td>-0.247221</td>\n",
       "      <td>-0.409374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.391514</td>\n",
       "      <td>0.452677</td>\n",
       "      <td>0.521407</td>\n",
       "      <td>0.595845</td>\n",
       "      <td>0.661691</td>\n",
       "      <td>0.702932</td>\n",
       "      <td>0.708613</td>\n",
       "      <td>0.682564</td>\n",
       "      <td>0.637765</td>\n",
       "      <td>deep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.044518</td>\n",
       "      <td>-1.935588</td>\n",
       "      <td>-1.808629</td>\n",
       "      <td>-1.667919</td>\n",
       "      <td>-1.513497</td>\n",
       "      <td>-1.348760</td>\n",
       "      <td>-1.171044</td>\n",
       "      <td>-0.972509</td>\n",
       "      <td>-0.759554</td>\n",
       "      <td>-0.547793</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138731</td>\n",
       "      <td>-0.053860</td>\n",
       "      <td>-0.241691</td>\n",
       "      <td>-0.417603</td>\n",
       "      <td>-0.582320</td>\n",
       "      <td>-0.738485</td>\n",
       "      <td>-0.889731</td>\n",
       "      <td>-1.037066</td>\n",
       "      <td>-1.174654</td>\n",
       "      <td>deep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.213535</td>\n",
       "      <td>-1.269056</td>\n",
       "      <td>-1.323306</td>\n",
       "      <td>-1.375251</td>\n",
       "      <td>-1.430062</td>\n",
       "      <td>-1.485479</td>\n",
       "      <td>-1.529200</td>\n",
       "      <td>-1.557172</td>\n",
       "      <td>-1.574662</td>\n",
       "      <td>-1.575457</td>\n",
       "      <td>...</td>\n",
       "      <td>0.947940</td>\n",
       "      <td>0.996154</td>\n",
       "      <td>1.035743</td>\n",
       "      <td>1.049543</td>\n",
       "      <td>1.024204</td>\n",
       "      <td>0.954716</td>\n",
       "      <td>0.844505</td>\n",
       "      <td>0.702445</td>\n",
       "      <td>0.541555</td>\n",
       "      <td>deep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.914806</td>\n",
       "      <td>-0.887726</td>\n",
       "      <td>-0.856065</td>\n",
       "      <td>-0.823527</td>\n",
       "      <td>-0.794551</td>\n",
       "      <td>-0.768074</td>\n",
       "      <td>-0.740895</td>\n",
       "      <td>-0.713364</td>\n",
       "      <td>-0.685445</td>\n",
       "      <td>-0.652020</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.478218</td>\n",
       "      <td>-0.571465</td>\n",
       "      <td>-0.684115</td>\n",
       "      <td>-0.817078</td>\n",
       "      <td>-0.966231</td>\n",
       "      <td>-1.122537</td>\n",
       "      <td>-1.264759</td>\n",
       "      <td>-1.376908</td>\n",
       "      <td>-1.461059</td>\n",
       "      <td>deep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.547469</td>\n",
       "      <td>-1.458818</td>\n",
       "      <td>-1.362120</td>\n",
       "      <td>-1.264829</td>\n",
       "      <td>-1.164948</td>\n",
       "      <td>-1.060064</td>\n",
       "      <td>-0.954496</td>\n",
       "      <td>-0.849448</td>\n",
       "      <td>-0.742812</td>\n",
       "      <td>-0.636614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227050</td>\n",
       "      <td>0.130983</td>\n",
       "      <td>0.041438</td>\n",
       "      <td>-0.038034</td>\n",
       "      <td>-0.106152</td>\n",
       "      <td>-0.163048</td>\n",
       "      <td>-0.210926</td>\n",
       "      <td>-0.253102</td>\n",
       "      <td>-0.290270</td>\n",
       "      <td>deep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26395</th>\n",
       "      <td>-0.152463</td>\n",
       "      <td>-0.164723</td>\n",
       "      <td>-0.165409</td>\n",
       "      <td>-0.152623</td>\n",
       "      <td>-0.118115</td>\n",
       "      <td>-0.066218</td>\n",
       "      <td>-0.010253</td>\n",
       "      <td>0.041637</td>\n",
       "      <td>0.092217</td>\n",
       "      <td>0.140510</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.336787</td>\n",
       "      <td>-0.306774</td>\n",
       "      <td>-0.280607</td>\n",
       "      <td>-0.269843</td>\n",
       "      <td>-0.260062</td>\n",
       "      <td>-0.229981</td>\n",
       "      <td>-0.167654</td>\n",
       "      <td>-0.082300</td>\n",
       "      <td>0.004372</td>\n",
       "      <td>quick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26396</th>\n",
       "      <td>-0.164723</td>\n",
       "      <td>-0.165409</td>\n",
       "      <td>-0.152623</td>\n",
       "      <td>-0.118115</td>\n",
       "      <td>-0.066218</td>\n",
       "      <td>-0.010253</td>\n",
       "      <td>0.041637</td>\n",
       "      <td>0.092217</td>\n",
       "      <td>0.140510</td>\n",
       "      <td>0.188025</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.306774</td>\n",
       "      <td>-0.280607</td>\n",
       "      <td>-0.269843</td>\n",
       "      <td>-0.260062</td>\n",
       "      <td>-0.229981</td>\n",
       "      <td>-0.167654</td>\n",
       "      <td>-0.082300</td>\n",
       "      <td>0.004372</td>\n",
       "      <td>0.089958</td>\n",
       "      <td>quick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26397</th>\n",
       "      <td>-0.165409</td>\n",
       "      <td>-0.152623</td>\n",
       "      <td>-0.118115</td>\n",
       "      <td>-0.066218</td>\n",
       "      <td>-0.010253</td>\n",
       "      <td>0.041637</td>\n",
       "      <td>0.092217</td>\n",
       "      <td>0.140510</td>\n",
       "      <td>0.188025</td>\n",
       "      <td>0.240939</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.280607</td>\n",
       "      <td>-0.269843</td>\n",
       "      <td>-0.260062</td>\n",
       "      <td>-0.229981</td>\n",
       "      <td>-0.167654</td>\n",
       "      <td>-0.082300</td>\n",
       "      <td>0.004372</td>\n",
       "      <td>0.089958</td>\n",
       "      <td>0.179209</td>\n",
       "      <td>quick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26398</th>\n",
       "      <td>-0.152623</td>\n",
       "      <td>-0.118115</td>\n",
       "      <td>-0.066218</td>\n",
       "      <td>-0.010253</td>\n",
       "      <td>0.041637</td>\n",
       "      <td>0.092217</td>\n",
       "      <td>0.140510</td>\n",
       "      <td>0.188025</td>\n",
       "      <td>0.240939</td>\n",
       "      <td>0.294399</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.269843</td>\n",
       "      <td>-0.260062</td>\n",
       "      <td>-0.229981</td>\n",
       "      <td>-0.167654</td>\n",
       "      <td>-0.082300</td>\n",
       "      <td>0.004372</td>\n",
       "      <td>0.089958</td>\n",
       "      <td>0.179209</td>\n",
       "      <td>0.264014</td>\n",
       "      <td>quick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26399</th>\n",
       "      <td>-0.118115</td>\n",
       "      <td>-0.066218</td>\n",
       "      <td>-0.010253</td>\n",
       "      <td>0.041637</td>\n",
       "      <td>0.092217</td>\n",
       "      <td>0.140510</td>\n",
       "      <td>0.188025</td>\n",
       "      <td>0.240939</td>\n",
       "      <td>0.294399</td>\n",
       "      <td>0.340346</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.260062</td>\n",
       "      <td>-0.229981</td>\n",
       "      <td>-0.167654</td>\n",
       "      <td>-0.082300</td>\n",
       "      <td>0.004372</td>\n",
       "      <td>0.089958</td>\n",
       "      <td>0.179209</td>\n",
       "      <td>0.264014</td>\n",
       "      <td>0.343418</td>\n",
       "      <td>quick</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25726 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0      0.483309  0.459790  0.431024  0.376565  0.295734  0.193290  0.066060   \n",
       "1     -2.044518 -1.935588 -1.808629 -1.667919 -1.513497 -1.348760 -1.171044   \n",
       "2     -1.213535 -1.269056 -1.323306 -1.375251 -1.430062 -1.485479 -1.529200   \n",
       "3     -0.914806 -0.887726 -0.856065 -0.823527 -0.794551 -0.768074 -0.740895   \n",
       "4     -1.547469 -1.458818 -1.362120 -1.264829 -1.164948 -1.060064 -0.954496   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "26395 -0.152463 -0.164723 -0.165409 -0.152623 -0.118115 -0.066218 -0.010253   \n",
       "26396 -0.164723 -0.165409 -0.152623 -0.118115 -0.066218 -0.010253  0.041637   \n",
       "26397 -0.165409 -0.152623 -0.118115 -0.066218 -0.010253  0.041637  0.092217   \n",
       "26398 -0.152623 -0.118115 -0.066218 -0.010253  0.041637  0.092217  0.140510   \n",
       "26399 -0.118115 -0.066218 -0.010253  0.041637  0.092217  0.140510  0.188025   \n",
       "\n",
       "              7         8         9  ...        76        77        78  \\\n",
       "0     -0.083445 -0.247221 -0.409374  ...  0.391514  0.452677  0.521407   \n",
       "1     -0.972509 -0.759554 -0.547793  ...  0.138731 -0.053860 -0.241691   \n",
       "2     -1.557172 -1.574662 -1.575457  ...  0.947940  0.996154  1.035743   \n",
       "3     -0.713364 -0.685445 -0.652020  ... -0.478218 -0.571465 -0.684115   \n",
       "4     -0.849448 -0.742812 -0.636614  ...  0.227050  0.130983  0.041438   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "26395  0.041637  0.092217  0.140510  ... -0.336787 -0.306774 -0.280607   \n",
       "26396  0.092217  0.140510  0.188025  ... -0.306774 -0.280607 -0.269843   \n",
       "26397  0.140510  0.188025  0.240939  ... -0.280607 -0.269843 -0.260062   \n",
       "26398  0.188025  0.240939  0.294399  ... -0.269843 -0.260062 -0.229981   \n",
       "26399  0.240939  0.294399  0.340346  ... -0.260062 -0.229981 -0.167654   \n",
       "\n",
       "             79        80        81        82        83        84  labels  \n",
       "0      0.595845  0.661691  0.702932  0.708613  0.682564  0.637765    deep  \n",
       "1     -0.417603 -0.582320 -0.738485 -0.889731 -1.037066 -1.174654    deep  \n",
       "2      1.049543  1.024204  0.954716  0.844505  0.702445  0.541555    deep  \n",
       "3     -0.817078 -0.966231 -1.122537 -1.264759 -1.376908 -1.461059    deep  \n",
       "4     -0.038034 -0.106152 -0.163048 -0.210926 -0.253102 -0.290270    deep  \n",
       "...         ...       ...       ...       ...       ...       ...     ...  \n",
       "26395 -0.269843 -0.260062 -0.229981 -0.167654 -0.082300  0.004372   quick  \n",
       "26396 -0.260062 -0.229981 -0.167654 -0.082300  0.004372  0.089958   quick  \n",
       "26397 -0.229981 -0.167654 -0.082300  0.004372  0.089958  0.179209   quick  \n",
       "26398 -0.167654 -0.082300  0.004372  0.089958  0.179209  0.264014   quick  \n",
       "26399 -0.082300  0.004372  0.089958  0.179209  0.264014  0.343418   quick  \n",
       "\n",
       "[25726 rows x 86 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yslyj_2KFzxZ",
    "outputId": "c7038aae-8ec4-4159-f3ea-a5af42978238"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normal        19060\n",
       "quick          2667\n",
       "hold           2133\n",
       "deep           1066\n",
       "deep_quick      800\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DarVtTeFfd_"
   },
   "source": [
    "### Separate the data according to their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Y_4XvpfzEYBR"
   },
   "outputs": [],
   "source": [
    "normal_df = df[df['labels'] == \"normal\"]\n",
    "quick_df = df[df['labels'] == \"quick\"]\n",
    "hold_df = df[df['labels'] == \"hold\"]\n",
    "deep_df = df[df['labels'] == \"deep\"]\n",
    "deep_quick_df = df[df['labels'] == \"deep_quick\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9hk--KOJNNx"
   },
   "source": [
    "### Seperate the data (X) and the label (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qi5q4DpoJLgr"
   },
   "outputs": [],
   "source": [
    "X = normal_df.iloc[:, :-1]\n",
    "Y = normal_df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYs6rxwqJESP"
   },
   "source": [
    "### Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HGMMQAxIJCbM"
   },
   "outputs": [],
   "source": [
    "# Data (X)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "YsPhN1DHJl0q"
   },
   "outputs": [],
   "source": [
    "# Label (Y)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# encode class values as integers [0,0,0,0,0,0,0,1,1,1,1,1,2,2,2,2]\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "hot_y = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ytoRT3LGmye"
   },
   "source": [
    "# PART 2 : Setup The Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DN95tLX5Gue"
   },
   "source": [
    "### Importing Neural Network Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "fdpxkFBm5GOO"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Activation\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DezqaGqwZFVe"
   },
   "source": [
    "### Neural Network : Deep Fully Connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "iDbiNLGFRR6k"
   },
   "outputs": [],
   "source": [
    "def AE():\n",
    "    model = Sequential()\n",
    "    # Encoder layers\n",
    "    model.add(Dense(64, input_shape=(85,)))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dense(16))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dense(8))\n",
    "    model.add(LeakyReLU())\n",
    "    \n",
    "    # Decoder layers\n",
    "    model.add(Dense(16, input_shape=(8,)))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dense(32))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dense(64))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dense(85))\n",
    "    model.add(Activation(activations.sigmoid))\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mae')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_lbAg1mK1pM"
   },
   "source": [
    "### GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cw9-7z23L39F",
    "outputId": "939e59a9-db14-4eea-bb91-23964a41982d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores: 4\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "\n",
    "print(f\"Number of CPU cores: {cpu_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88IHXBJFMufL",
    "outputId": "2f52e1a0-5b07-46a1-a986-ae92bed046b9"
   },
   "outputs": [],
   "source": [
    "!pip install scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "xkCeGQooK0a0"
   },
   "outputs": [],
   "source": [
    "model_kr = KerasRegressor(model=AE)\n",
    "\n",
    "# Define the hyperparameters and their values to search\n",
    "param_grid = {\n",
    "    'epochs': [10],\n",
    "    'batch_size': [32]\n",
    "}\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(estimator=model_kr, param_grid=param_grid, cv=2, verbose=5, refit=True, n_jobs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzoHxtnb8CU-"
   },
   "source": [
    "## PART 3 : Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K-xmwYeUNRgH",
    "outputId": "ecb2fb71-8a0a-4fb1-ce3d-afae40b36e10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Epoch 1/10\n",
      "596/596 [==============================] - 4s 4ms/step - loss: 0.0425\n",
      "Epoch 2/10\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.0286\n",
      "Epoch 3/10\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.0235\n",
      "Epoch 4/10\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.0228\n",
      "Epoch 5/10\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 0.0218\n",
      "Epoch 6/10\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.0205\n",
      "Epoch 7/10\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.0200\n",
      "Epoch 8/10\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.0197\n",
      "Epoch 9/10\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.0195\n",
      "Epoch 10/10\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 0.0192\n",
      "Best Hyperparameters: {'batch_size': 32, 'epochs': 10}\n",
      "Best MSE: -0.8330229373056475\n"
     ]
    }
   ],
   "source": [
    "# Fit the grid search object to your data\n",
    "grid_search.fit(X, X)  # Assuming X_train is your training data\n",
    "\n",
    "# Print the best hyperparameters and the corresponding mean squared error\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best MSE:\", -grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KerasRegressor(\n",
       "\tmodel=<function AE at 0x000001C1A65A5430>\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=32\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=10\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "596/596 [==============================] - 4s 4ms/step - loss: 0.0398\n",
      "Epoch 2/10\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.0273\n",
      "Epoch 3/10\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.0233\n",
      "Epoch 4/10\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.0229\n",
      "Epoch 5/10\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.0220\n",
      "Epoch 6/10\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 0.0204\n",
      "Epoch 7/10\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 0.0199\n",
      "Epoch 8/10\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.0197\n",
      "Epoch 9/10\n",
      "596/596 [==============================] - 2s 4ms/step - loss: 0.0195\n",
      "Epoch 10/10\n",
      "596/596 [==============================] - 3s 4ms/step - loss: 0.0193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c1a7fec220>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_model = AE()\n",
    "main_model.fit(X, X, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the encoder model using the trained weights\n",
    "encoder_output = main_model.layers[8].output\n",
    "encoder_model = Model(inputs=main_model.input, outputs=encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = encoder_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CeGHumdKMDFY",
    "outputId": "45edbbf0-3f52-4486-c633-fe759c69ccef"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "model = AE()\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mae')\n",
    "\n",
    "history = model.fit(X, X,\n",
    "          epochs=20,\n",
    "          batch_size=32,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8VW_0go9FMV"
   },
   "source": [
    "### Plot the training loss graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "xBc394dYNfGI",
    "outputId": "a3ebae64-3006-4be5-bc3a-63c5f5363345"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCtO12xs8f_1"
   },
   "source": [
    "### Define a function to smoothing the wave curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pptqXcSN8a-l"
   },
   "outputs": [],
   "source": [
    "# smoothing the wave of decoded_data\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "def smooth_wave(wave):\n",
    "  # Define the parameters for the Savitzky-Golay filter\n",
    "  window_length = 10  # The length of the window (odd number)\n",
    "  polyorder = 2  # The order of the polynomial fit\n",
    "\n",
    "  return savgol_filter(wave, window_length, polyorder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFCQHzRs9H1F"
   },
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Uu_aBhlOulH",
    "outputId": "2dc99771-4d4c-43cf-b089-caf182c6d1d6"
   },
   "outputs": [],
   "source": [
    "#encoded_data = model.encoder(X).numpy()\n",
    "#decoded_data = model.decoder(encoded_data).numpy()\n",
    "\n",
    "decoded_data = model.predict(X)\n",
    "\n",
    "# Apply the Savitzky-Golay filter\n",
    "decoded_data = smooth_wave(decoded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIImf_wE9eZQ"
   },
   "source": [
    "### Calculate the Mean Average Error (MAE) from all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "29Ixn4SR_ZDd",
    "outputId": "71694deb-06b7-40cf-aa2c-07ad20e559b9"
   },
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.mae(decoded_data, X)\n",
    "print(\"Mean Average Error : {}\".format(np.mean(loss * 100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Uc6vII_TU0H"
   },
   "source": [
    "### Plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862
    },
    "id": "dAdFN4B2-ER6",
    "outputId": "d2a462d8-e6b2-4fd6-ba1b-fb2251e9d379"
   },
   "outputs": [],
   "source": [
    "num_samples = 20\n",
    "row = int(num_samples / 5)\n",
    "\n",
    "# Create figure and axis objects\n",
    "fig, ax = plt.subplots(row, 5, figsize=(20,row*3))\n",
    "\n",
    "idx=0\n",
    "for y in range(row):\n",
    "  for x in range(5):\n",
    "    # Plot each time series\n",
    "    ax[y, x].plot(X[idx], 'b')\n",
    "    ax[y, x].plot(decoded_data[idx], 'g')\n",
    "    ax[y, x].fill_between(np.arange(X.shape[1]), decoded_data[idx], X[idx], color='lightcoral')\n",
    "    ax[y, x].set_title(\"Data {}; err : {:.2f}%\".format(idx, loss[idx]*100))\n",
    "    #ax[y, x].legend()\n",
    "\n",
    "    idx += 1\n",
    "\n",
    "# legend\n",
    "plt.legend(labels=[\"Input\", \"Reconstruction\", \"Error\"], loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Customize the overall layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BH6mnrbz4jI"
   },
   "source": [
    "## PART 4 : Generating New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ob4C1KuQPIOR",
    "outputId": "0bb6b410-8c2e-4512-e61f-b867229db046"
   },
   "outputs": [],
   "source": [
    "# Set the number of data points to generate\n",
    "num_samples = 10\n",
    "\n",
    "# Randomly sample latent vectors from a predefined range\n",
    "noise_vectors = np.random.rand(*(num_samples, 85))\n",
    "\n",
    "# Generate new data by decoding the latent vectors\n",
    "generated_data = model.predict(noise_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "id": "yEVgCWga0GHF",
    "outputId": "d65e02c9-9577-4804-f847-8bc8ad56baa4"
   },
   "outputs": [],
   "source": [
    "# Create figure and axis objects\n",
    "row = int(num_samples / 5)\n",
    "\n",
    "fig, ax = plt.subplots(row, 5, figsize=(20,row*3))\n",
    "\n",
    "idx=0\n",
    "for y in range(row):\n",
    "  for x in range(5):\n",
    "    # Plot each time series\n",
    "    ax[y, x].plot(generated_data[idx], 'b')\n",
    "    ax[y, x].set_title(\"Data {}\".format(idx))\n",
    "\n",
    "    idx += 1\n",
    "\n",
    "# legend\n",
    "plt.legend(labels=[\"Generated\"], loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# title\n",
    "plt.suptitle(\"Generated Data without Smoothing\")\n",
    "\n",
    "# Customize the overall layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFGqIT4_87kE"
   },
   "source": [
    "## Smoothing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uvs6rFeV0eO7"
   },
   "outputs": [],
   "source": [
    "# Apply the Savitzky-Golay filter\n",
    "generated_data = smooth_wave(generated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "id": "gSIuhXB-9B69",
    "outputId": "ddac4beb-d3c6-4b46-cb63-de07bd213539"
   },
   "outputs": [],
   "source": [
    "# Create figure and axis objects\n",
    "row = int(num_samples / 5)\n",
    "\n",
    "fig, ax = plt.subplots(row, 5, figsize=(20,row*3))\n",
    "\n",
    "idx=0\n",
    "for y in range(row):\n",
    "  for x in range(5):\n",
    "    # Plot each time series\n",
    "    ax[y, x].plot(generated_data[idx], 'b')\n",
    "    ax[y, x].set_title(\"Data {}\".format(idx))\n",
    "\n",
    "    idx += 1\n",
    "\n",
    "# legend\n",
    "plt.legend(labels=[\"Generated\"], loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# title\n",
    "plt.suptitle(\"Generated Data with Smoothing\")\n",
    "\n",
    "# Customize the overall layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
